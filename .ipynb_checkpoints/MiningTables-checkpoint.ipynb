{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044daaf5",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.imgur.com/zRrFdsf.png\" width=\"700\"></center>\n",
    "\n",
    "# Analytics on GeodataFrames\n",
    "\n",
    "Up to now, we have work with the spatial information only. This is time to use  local indicators (social, economic, physical, etc.)  at each spatial location to produce some analytics. That is possible through a series of steps:\n",
    "\n",
    "1. Preprocessing:\n",
    "    - the dataframe.\n",
    "    - geodataframe.\n",
    "2. Merging info fro DataFrames into the maps (a GeoDataFrame).\n",
    "3. Exploring the Data.\n",
    "\n",
    "Let's create a repo in with GitHub. Put this [file](https://drive.google.com/file/d/1EYacndGCRiF1ZHEnGa-avTXSEtqB2e7p/view?usp=sharing) in the maps folder, and this other [file](https://docs.google.com/spreadsheets/d/1xpsz9n-SBTwgtXsugmabpBJ-tCaEwg9_/edit?usp=sharing&ouid=106935788518947165917&rtpof=true&sd=true) in the data folder. \n",
    "\n",
    "Let's read the data in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31450dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data table\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "datadis=pd.read_excel(os.path.join('data','dataPeru_indicadores.xlsx'),\n",
    "                     dtype={'Ubigeo': object})\n",
    "datadis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "datadismap=gpd.read_file(os.path.join('maps','DistritosMap.zip'))\n",
    "\n",
    "datadismap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1294312",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "After observing both tables, it would be better if the columns with names have the same capitalization, and no extra blank spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitalizeColumns=lambda x: x.str.upper().str.strip()\n",
    "datadis[['Provincia','Distrito']]=datadis[['Provincia','Distrito']].apply(capitalizeColumns)\n",
    "datadismap[['PROVINCIA','DISTRITO']]=datadismap[['PROVINCIA','DISTRITO']].apply(capitalizeColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85ce45",
   "metadata": {},
   "source": [
    "The names from non-english speaking countries may come with some symbols that may cause trouble (', ~). Let's get rid of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16981570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "\n",
    "byePunctuation=lambda x: unidecode.unidecode(x)\n",
    "datadis[['Provincia','Distrito']]=datadis[['Provincia','Distrito']].applymap(byePunctuation)\n",
    "datadismap[['PROVINCIA','DISTRITO']]=datadismap[['PROVINCIA','DISTRITO']].applymap(byePunctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ffa80",
   "metadata": {},
   "source": [
    "Let me see how many district we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d789441",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datadis.Distrito),len(datadismap.DISTRITO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adec7c2",
   "metadata": {},
   "source": [
    "Are the name of the districts unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaedf0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadis.Distrito.duplicated().sum(),datadismap.DISTRITO.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5e007",
   "metadata": {},
   "source": [
    "The presence of duplicates, forces we create  a column of unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d932ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating\n",
    "datadis['provDist']=[\"+\".join(pd) for pd in zip (datadis.Provincia,datadis.Distrito)]\n",
    "datadismap['provDist']=[\"+\".join(pd) for pd in zip (datadismap.PROVINCIA,datadismap.DISTRITO)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c643838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new column looks like this:\n",
    "datadis['provDist'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a93252",
   "metadata": {},
   "source": [
    "It would be good making sure no *ghost* appears between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing dashes and multiple spaces by a simple space\n",
    "datadis.provDist=datadis.provDist.str.replace(\"\\-|\\_|\\s+\",\" \",regex=True)\n",
    "datadismap.provDist=datadismap.provDist.str.replace(\"\\-|\\_|\\s+\",\" \",regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406417d",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "We need to merge both tables now. That can happen effectively if both tables have a **key** column: a column (or collection of them) whose values in one table are the same in the other one.\n",
    "\n",
    "The match need not be exact, but only common values in the *key* are merged.\n",
    "\n",
    "Let's find out what is NOT matched in each table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793335e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomatch_df=set(datadis.provDist)- set(datadismap.provDist)\n",
    "nomatch_gdf=set(datadismap.provDist)-set(datadis.provDist) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849b640",
   "metadata": {},
   "source": [
    "This is what could not be matched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada23390",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nomatch_df), len(nomatch_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c4a60",
   "metadata": {},
   "source": [
    "The datadis df does not have 26 values in its provDist column that match in the provDist column in datadismap gdf, the same count happens the way around. \n",
    "\n",
    "You can try renaming here row by row (or in the original file), but the right way to go is using **fuzzy merging**, please install _the fuzz_ and _python-Levenshtein_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the closest match from nomatch_gdf for a value in nomatch_df\n",
    "from thefuzz import process\n",
    "[(dis,process.extractOne(dis,nomatch_gdf)) for dis in sorted(nomatch_df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b462ab3",
   "metadata": {},
   "source": [
    "If you are comfortable, you prepare a _dictionary_ of changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is this OK?\n",
    "{dis:process.extractOne(dis,nomatch_gdf)[0] for dis in sorted(nomatch_df)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then:\n",
    "changesDis_df={dis:process.extractOne(dis,nomatch_gdf)[0] for dis in sorted(nomatch_df)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940be9cd",
   "metadata": {},
   "source": [
    "Now, make the replacements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39fb674",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadis.provDist.replace(changesDis_df,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd65363",
   "metadata": {},
   "source": [
    "Now the merge can happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadismap=datadismap.merge(datadis, on='provDist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "datadismap.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142c0b7",
   "metadata": {},
   "source": [
    "We can get rid of some columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bye=['Departamento', 'Provincia', 'Distrito','INSTITUCIO']\n",
    "datadismap.drop(columns=bye,inplace=True)\n",
    "\n",
    "# keeping\n",
    "datadismap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff93fc",
   "metadata": {},
   "source": [
    "We can save this gdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c97517",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadismap.to_file(os.path.join('maps',\"dataMapPeru.gpkg\"), layer='distritos', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b100004",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<div class=\"alert-success\">\n",
    "    \n",
    "**ALTERNATIVE A**\n",
    "    \n",
    "1. Get a map of the Provinces of Peru.\n",
    "    \n",
    "2. Get a table of variables on provinces of Peru. At least 3 numerical variables.\n",
    "\n",
    "3. Preprocess both tables and get them ready for merging.\n",
    "\n",
    "4. Do the merging, making the changes needed so that you keep the most columns.\n",
    "    \n",
    "\n",
    "**ALTERNATIVE B**    \n",
    "    \n",
    "1. Use the map of the Districts of Peru from this class.\n",
    "    \n",
    "2. Get a new table of variables on districts of Peru. At least 3 numerical variables.\n",
    "\n",
    "3. Preprocess both tables and get them ready for merging.\n",
    "\n",
    "4. Do the merging, making the changes needed so that you keep the most columns.\n",
    "\n",
    "5. Select the districts of ONE Department for the other excercises. Do not select  Pasco,Tacna,Moquegua, Ucayali, Tumbes, Madre de Dios, Callao.\n",
    "\n",
    "    \n",
    "**ALTERNATIVE C**\n",
    "\n",
    "Do alternative A with a different country.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b518c79",
   "metadata": {},
   "source": [
    "## Exploring one variable\n",
    "\n",
    "This time, we explore statistically one variable in the map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics\n",
    "datadismap.IDH2019.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sea\n",
    "\n",
    "sea.displot(datadismap.IDH2019, color='yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31653498",
   "metadata": {},
   "source": [
    "This plot tells you the distribution of the values, but not the presence of outliers, which you are revealed in a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea.boxplot(datadismap.IDH2019, color='yellow',orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee802b",
   "metadata": {},
   "source": [
    "Notice the histogram divides the data in intervals which are the base of the bars. Seaborn uses the [Freedman-Diaconis](https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule) formula to compute the bins.\n",
    "\n",
    "Let's see other possibilities, but please install [**numba**](https://numba.readthedocs.io/en/stable/user/installing.html) before runing the next code; also make sure you have **pysal**, **mapclassify** and **numpy** installed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mapclassify \n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12345) # so we all get the same results!\n",
    "\n",
    "# let's try 5 intervals\n",
    "K=5\n",
    "# same interval width, easy interpretation\n",
    "ei5 = mapclassify.EqualInterval(datadismap['IDH2019'], k=K)\n",
    "# same interval width based on standard deviation, easy - but not as the previous one, poor when high skewness\n",
    "msd = mapclassify.StdMean(datadismap['IDH2019'])\n",
    "# interval width varies, counts per interval are close, not easy to grasp, repeated values complicate cuts                                \n",
    "q5=mapclassify.Quantiles(datadismap['IDH2019'],k=K)\n",
    "\n",
    "# based on similarity, good for multimodal data \n",
    "mb5 = mapclassify.MaximumBreaks(datadismap['IDH2019'], k=K)\n",
    "# based on similarity, good for skewed data\n",
    "ht = mapclassify.HeadTailBreaks(datadismap['IDH2019']) # no K needed\n",
    "# based on similarity, optimizer\n",
    "fj5 = mapclassify.FisherJenks(datadismap['IDH2019'], k=K)\n",
    "# based on similarity, optimizer\n",
    "jc5 = mapclassify.JenksCaspall(datadismap['IDH2019'], k=K)\n",
    "# based on similarity, optimizer\n",
    "mp5 = mapclassify.MaxP(datadismap['IDH2019'], k=K)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c2ad4",
   "metadata": {},
   "source": [
    "Let's see the **HeadTailBreaks** results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f71151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a frequency table by default\n",
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce570a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group label\n",
    "ht.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels and counts\n",
    "np.unique(ht.yb,return_counts=True)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ht.yb into a pandas Series\n",
    "\n",
    "pd.Series(ht.yb).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee200de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the cuts, but it is not including the min value\n",
    "ht.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d0bac",
   "metadata": {},
   "source": [
    "Based on the previous information, let me prepare a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aef9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completing the bins\n",
    "HT_bins=list(ht.bins)\n",
    "HT_bins.insert(0,datadismap.IDH2019.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55862575",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea.displot(datadismap.IDH2019, bins=HT_bins,color='yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37602d2b",
   "metadata": {},
   "source": [
    "How can we select the right classification. Let me use the the Absolute deviation around class median (ADCM) to make the comparisson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class5 = q5, ei5,msd, ht, mb5, fj5, jc5, mp5\n",
    "# Collect ADCM for each classifier\n",
    "fits = np.array([ c.adcm for c in class5])\n",
    "# Convert ADCM scores to a DataFrame\n",
    "adcms = pd.DataFrame(fits)\n",
    "# Add classifier names\n",
    "adcms['classifier'] = [c.name for c in class5]\n",
    "# Add column names to the ADCM\n",
    "adcms.columns = ['ADCM', 'Classifier']\n",
    "ax = sea.barplot(\n",
    "    y='Classifier', x='ADCM', data=adcms, palette='Pastel1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec883be",
   "metadata": {},
   "source": [
    "Let's keep the three schemes with the lowest  ADCM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadismap['QU'] = q5.yb \n",
    "datadismap['FJ'] = fj5.yb\n",
    "datadismap['JC'] = jc5.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ddf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many cases per scheme:\n",
    "\n",
    "class5new = q5,fj5, jc5\n",
    "pd.DataFrame(\n",
    "    {c.name: c.counts for c in class5new},\n",
    "    index=['Class-{}'.format(i) for i in range(5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4163499",
   "metadata": {},
   "source": [
    "Let me plot one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "datadismap.plot(column='QU', \n",
    "        cmap='viridis', \n",
    "        categorical=True,\n",
    "        edgecolor='white', \n",
    "        linewidth=0., \n",
    "        alpha=0.75, \n",
    "        legend=True,\n",
    "        legend_kwds=dict(loc=2),\n",
    "        ax=ax\n",
    "       )\n",
    "\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff92c74",
   "metadata": {},
   "source": [
    "Notice the **geopandas.plot()** can also use those schemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "datadismap.plot(column='IDH2019', \n",
    "        cmap='viridis',       \n",
    "        scheme='Quantiles',\n",
    "        k=5, \n",
    "        edgecolor='white', \n",
    "        linewidth=0., \n",
    "        alpha=0.75, \n",
    "        legend=True,\n",
    "        legend_kwds=dict(loc=2),\n",
    "        ax=ax\n",
    "       )\n",
    "\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a642147",
   "metadata": {},
   "source": [
    "Let's save what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update\n",
    "datadismap.to_file(os.path.join('maps',\"dataMapPeru.gpkg\"), layer='distritos', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd514b81",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "<div class=\"alert-success\">\n",
    "    \n",
    "1. Choose a numeric variable from your merged data.\n",
    "2. Decide which are the three best classification schemes for that variable.\n",
    "3. Make a map for each scheme selected.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba06e47",
   "metadata": {},
   "source": [
    "# Exploring several variables\n",
    "\n",
    "We can turn our attention to several variables now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87283d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables = ['Educ_sec_comp2019_pct',\n",
    "                     'NBI2017_pct', \n",
    "                     'Viv_sin_serv_hig2017_pct']\n",
    "datadismap[selected_variables].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d1eb2",
   "metadata": {},
   "source": [
    "Visualization in a key procedure to detect the need for further data transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b8695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea.boxplot(datadismap[selected_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5beb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadismap[selected_variables].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb06a04",
   "metadata": {},
   "source": [
    "Both univariate plots do not allow to see the relationships. Let's try a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733353c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea.pairplot(\n",
    "    datadismap[selected_variables], kind=\"reg\", diag_kind=\"kde\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708cbe7",
   "metadata": {},
   "source": [
    "We finally realize that **Educ_sec_comp2019_pct** has a negative correlation with the others (as expected). Let me reverse the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse=lambda x:abs(x-x.max())+x.min()\n",
    "\n",
    "datadismap['Educ_sec_NO_comp2019_pct']=reverse(datadismap['Educ_sec_comp2019_pct'])\n",
    "datadismap['Educ_sec_NO_comp2019_pct'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf024f",
   "metadata": {},
   "source": [
    "Now all our correlations will be positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4296551",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables_new = ['Educ_sec_NO_comp2019_pct',\n",
    "                     'NBI2017_pct', \n",
    "                     'Viv_sin_serv_hig2017_pct']\n",
    "sea.pairplot(\n",
    "    datadismap[selected_variables_new], kind=\"reg\", diag_kind=\"kde\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334fef34",
   "metadata": {},
   "source": [
    "Keep in mind that the boxplot showed outliers. Let me try to re scale the data to smooth the distribution a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToScale=selected_variables_new\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import robust_scale as rs\n",
    "from sklearn.preprocessing import power_transform as pt\n",
    "from sklearn.preprocessing import quantile_transform as qt\n",
    "\n",
    "rs_result=rs(datadismap[columnsToScale])\n",
    "pt_result=pt(datadismap[columnsToScale])\n",
    "qt_result=qt(datadismap[columnsToScale])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e10c52",
   "metadata": {},
   "source": [
    "Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rs_result).boxplot(vert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pt_result).boxplot(vert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1120e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(qt_result).boxplot(vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d0042",
   "metadata": {},
   "source": [
    "I will keep the **qt_result**, so let me add some columns to the gdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6725e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new names\n",
    "selected_variables_new_t=[s+'_t' for s in selected_variables_new ]\n",
    "\n",
    "# add colunms\n",
    "datadismap[selected_variables_new_t]=qt_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd99ff6",
   "metadata": {},
   "source": [
    "Let's plot the data we have used (I will the quantiles scheme **without** pre evaluation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots( ncols=3, nrows=3,figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "# Start a loop over all the variables of interest\n",
    "for i, col in enumerate(selected_variables_new_t + selected_variables_new + selected_variables):\n",
    "    # select the axis where the map will go\n",
    "    ax = axs[i]\n",
    "    # Plot the map\n",
    "    datadismap.plot(\n",
    "        column=col,\n",
    "        ax=ax,\n",
    "        scheme=\"Quantiles\",\n",
    "        linewidth=0,\n",
    "        cmap=\"RdPu\",\n",
    "    )\n",
    "    # Remove axis clutter\n",
    "    ax.set_axis_off()\n",
    "    # Set the axis title to the name of variable being plotted\n",
    "    ax.set_title(col)\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update\n",
    "datadismap.to_file(os.path.join('maps',\"dataMapPeru.gpkg\"), layer='distritos', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51843ff9",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "<div class=\"alert-success\">\n",
    "    \n",
    "1. Select three numeric variables.\n",
    "    \n",
    "2. Explore each one as above:\n",
    "    * If needed reverse the variables (so that correlations are positive)\n",
    "    * If needed re scale the variables (so that outliers are smoothed)\n",
    "    * Evalue the  classification schemes for each variable.\n",
    "\n",
    "4. Prepare a map for each variable, using the best classification scheme for each variable.\n",
    "\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1aa3f",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We used clustering to organize the data into homogeneus groups. Let me use a **dendogram** to explore the possible clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49004eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy as hc\n",
    "\n",
    "\n",
    "Z = hc.linkage(qt_result, 'ward')\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('cases')\n",
    "plt.ylabel('distance')\n",
    "hc.dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=1,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8324827",
   "metadata": {},
   "source": [
    "I have used all the variables in the dendogram, so I can decide how many groups could be created using that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering as agnes\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "# Initialize the algorithm, requesting 3 clusters\n",
    "model = agnes(linkage=\"ward\", n_clusters=3).fit(datadismap[selected_variables_new_t])\n",
    "# Assign labels to main data table\n",
    "datadismap[\"hc_3\"] = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a9175",
   "metadata": {},
   "source": [
    "I reduce all the information from the three variables into a cluster variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadismap[\"hc_3\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f17548",
   "metadata": {},
   "source": [
    "Let's use the mean of each variable to understand the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadismap.groupby(\"hc_3\")[selected_variables_new].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150d66e",
   "metadata": {},
   "source": [
    "I could profile the clusters better if I prepare a plot with all the variables. Let me create a new data frame in long shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index db on cluster ID\n",
    "datadismap_long = datadismap.set_index(\"hc_3\").copy()\n",
    "# Keep only variables used for clustering\n",
    "datadismap_long = datadismap_long[selected_variables_new]\n",
    "# Stack column names into a column, obtaining\n",
    "# a \"long\" version of the dataset\n",
    "datadismap_long = datadismap_long.stack()\n",
    "# Take indices into proper columns\n",
    "datadismap_long = datadismap_long.reset_index()\n",
    "# Rename column names\n",
    "datadismap_long = datadismap_long.rename(\n",
    "    columns={\"level_1\": \"Attribute\", 0: \"Values\"}\n",
    ")\n",
    "# Check out result\n",
    "datadismap_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1685c19e",
   "metadata": {},
   "source": [
    "Nos the profiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b954349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the facets\n",
    "facets = sea.FacetGrid(\n",
    "    data=datadismap_long,\n",
    "    col=\"hc_3\",\n",
    "    hue=\"Attribute\",\n",
    "    sharey=False,\n",
    "    sharex=False,\n",
    "    aspect=2,\n",
    "    col_wrap=3,\n",
    ")\n",
    "# Build the plot as a `sns.kdeplot`\n",
    "facets.map(sea.kdeplot, \"Values\", fill=True).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7eabd9",
   "metadata": {},
   "source": [
    "Finally, just color the map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot unique values choropleth including\n",
    "# a legend and with no boundary lines\n",
    "datadismap.plot(\n",
    "    column=\"hc_3\", categorical=True, legend=True, linewidth=0, ax=ax\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05081d92",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "<div class=\"alert-success\">\n",
    "    \n",
    "1. Keep the previous three variables, after you pre processed them.\n",
    "    \n",
    "2. Prepare a dendogram, and decide which is better: 3 or 5 clusters.\n",
    "    \n",
    "3. Cluster the data into the amount of clusters selected.\n",
    "\n",
    "4. Prepare a visualization to profile each cluster.\n",
    "\n",
    "5. Plot the cluster map.\n",
    "\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87708d4e",
   "metadata": {},
   "source": [
    "## Spatial Correlation\n",
    "\n",
    "### Neighboorhood\n",
    "\n",
    "We can compute the neighborhood in a map using different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0152188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.weights import Queen, Rook, KNN\n",
    "\n",
    "# rook\n",
    "w_rook = Rook.from_dataframe(datadismap) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282be4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rook\n",
    "w_queen = Queen.from_dataframe(datadismap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k nearest neighbors\n",
    "w_knn = KNN.from_dataframe(datadismap, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7c94a",
   "metadata": {},
   "source": [
    "Let's understand the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first one\n",
    "datadismap.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd010e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount neighbors of that district\n",
    "w_rook.neighbors[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d9e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see\n",
    "base=datadismap[datadismap.PROVINCIA==\"TACNA\"].plot()\n",
    "datadismap.iloc[w_rook.neighbors[0] ,].plot(ax=base,facecolor=\"yellow\")\n",
    "datadismap.head(1).plot(ax=base,facecolor=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee5594",
   "metadata": {},
   "source": [
    "Let's do the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a94ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_queen.neighbors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959642c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base=datadismap[datadismap.PROVINCIA==\"TACNA\"].plot()\n",
    "datadismap.iloc[w_queen.neighbors[0] ,].plot(ax=base,facecolor=\"yellow\")\n",
    "datadismap.head(1).plot(ax=base,facecolor=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_knn.neighbors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base=datadismap[datadismap.PROVINCIA==\"TACNA\"].plot()\n",
    "datadismap.iloc[w_knn.neighbors[0],].plot(ax=base,facecolor=\"yellow\")\n",
    "datadismap.head(1).plot(ax=base,facecolor=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c4f9d",
   "metadata": {},
   "source": [
    "Let me pay attention to the knn results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the neighbors by row\n",
    "w_knn.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the matrix of neighboorhood:\n",
    "\n",
    "pd.DataFrame(*w_knn.full()).astype(int) # 1 means both are neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of zeros\n",
    "w_knn.nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4395652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct of neighboorhood (density)\n",
    "w_knn.pct_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e318d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a district with NO neighbor?\n",
    "w_knn.islands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc26bd2",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "<div class=\"alert-success\">\n",
    "    \n",
    "1. Compute the three neighboohoods shown above for you data.\n",
    "    \n",
    "2. Select one polygon, and plot it with its neighbors as above.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28192fdf",
   "metadata": {},
   "source": [
    "## Spatial correlation\n",
    "\n",
    "We need the neighboorhood matrix (the weight matrix) to compute spatial correlation: if the variable value is correlated with the values of its neighbors - which proves a spatial effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for spatial correlation\n",
    "w_knn.transform = 'R'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91941edd",
   "metadata": {},
   "source": [
    "Spatial correlation is measured by the Moran's I statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda.moran import Moran\n",
    "\n",
    "moranIDH = Moran(datadismap['IDH2019'], w_knn)\n",
    "moranIDH.I,moranIDH.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b450c",
   "metadata": {},
   "source": [
    "A significant Moran's I suggest spatial correlation. Let's see the spatial scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splot.esda import moran_scatterplot\n",
    "\n",
    "fig, ax = moran_scatterplot(moranIDH, aspect_equal=True)\n",
    "ax.set_xlabel('IDH_std')\n",
    "ax.set_ylabel('SpatialLag_IDH_std')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec77ca",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "<div class=\"alert-success\">\n",
    "    \n",
    "1. Compute the Moran's coefficient for **all** your numeric variables.\n",
    "    \n",
    "2. Make a scatter plot for each variable.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196148f8",
   "metadata": {},
   "source": [
    "### Local Spatial Correlation\n",
    "\n",
    "We can compute a LISA (local Moran) for each case. That will help us find spatial clusters (spots) and spatial outliers:\n",
    "\n",
    "* A **hotSpot** is a polygon whose value in the variable is high AND is surrounded with polygons with also high values.\n",
    "\n",
    "* A **coldSpot** is a polygon whose value in the variable is low AND is surrounded with polygons with also low values.\n",
    "\n",
    "* A **coldOutlier** is a polygon whose value in the variable is low BUT is surrounded with polygons with  high values.\n",
    "\n",
    "* A **hotOutlier** is a polygon whose value in the variable is high BUT is surrounded with polygons with  low values.\n",
    "\n",
    "It is also possible that no significant correlation is detected. Let's see those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1207a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scatterplot with local info\n",
    "from esda.moran import Moran_Local\n",
    "\n",
    "# calculate Moran_Local and plot\n",
    "lisaIDH = Moran_Local(y=datadismap['IDH2019'], w=w_knn,seed=2022)\n",
    "fig, ax = moran_scatterplot(lisaIDH,p=0.05)\n",
    "ax.set_xlabel('IDH_std')\n",
    "ax.set_ylabel('SpatialLag_IDH_std')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the map with the spots and outliers\n",
    "\n",
    "from splot.esda import lisa_cluster\n",
    "f, ax = plt.subplots(1, figsize=(12, 12))\n",
    "plt.title('Spots and Outliers')\n",
    "fig = lisa_cluster(lisaIDH, \n",
    "                   datadismap,ax=ax,\n",
    "                   legend_kwds={'loc': 'center left', \n",
    "                                'bbox_to_anchor': (0.7, 0.6)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d157e",
   "metadata": {},
   "source": [
    "Let me add that data to my gdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67766a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadrant\n",
    "lisaIDH.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8949df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance\n",
    "lisaIDH.p_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadrant: 1 HH,  2 LH,  3 LL,  4 HL\n",
    "pd.Series(lisaIDH.q).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4c66f",
   "metadata": {},
   "source": [
    "The info in **lisaIDH.q** can not be used right away, we need to add if the local spatial correlation is significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2977e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadismap['IDH_quadrant']=[l if p <0.05 else 0 for l,p in zip(lisaIDH.q,lisaIDH.p_sim)  ]\n",
    "datadismap['IDH_quadrant'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f327eb6f",
   "metadata": {},
   "source": [
    "Now, we recode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd49705",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ '0 no_sig', '1 hotSpot', '2 coldOutlier', '3 coldSpot', '4 hotOutlier']\n",
    "\n",
    "datadismap['IDH_quadrant_names']=[labels[i] for i in datadismap['IDH_quadrant']]\n",
    "\n",
    "datadismap['IDH_quadrant_names'].value_counts()\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b662813",
   "metadata": {},
   "source": [
    "Let's replot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bad5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "myColMap = colors.ListedColormap([ 'ghostwhite', 'red', 'green', 'black','orange'])\n",
    "\n",
    "\n",
    "\n",
    "# Set up figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(12,12))\n",
    "# Plot unique values choropleth including\n",
    "# a legend and with no boundary lines\n",
    "\n",
    "plt.title('Spots and Outliers')\n",
    "\n",
    "datadismap.plot(column='IDH_quadrant_names', \n",
    "                categorical=True,\n",
    "                cmap=myColMap,\n",
    "                linewidth=0.1, \n",
    "                edgecolor='white',\n",
    "                legend=True,\n",
    "                legend_kwds={'loc': 'center left', \n",
    "                             'bbox_to_anchor': (0.7, 0.6)},\n",
    "                ax=ax)\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final update\n",
    "datadismap.to_file(os.path.join('maps',\"dataMapPeru.gpkg\"), layer='distritos', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe389a6",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "<div class=\"alert-success\">\n",
    "    \n",
    "1. Compute the Local Moran for the variables in your data that have significant spatial correlation.\n",
    "    \n",
    "2. Create a new column for each of those variables, with a label ('0 no_sig', '1 hotSpot', '2 coldOutlier', '3 coldSpot', '4 hotOutlier').\n",
    "\n",
    "3. Prepare a map for each of the variables analyzed, showing the spots and outliers.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02d3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
