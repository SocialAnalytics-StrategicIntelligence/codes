{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/6U6q5jQ.png\"/>\n",
    "_____\n",
    "\n",
    "<a id='home'></a>\n",
    "\n",
    "# Merging and Choroplet maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merging process seems like a simple one: you take two tables and make one.\n",
    "However, in this session I am going to show you a case where several issues arise on your way to merge tables, where one of them will be a map.\n",
    "\n",
    "I will cover some important processes for tables:\n",
    "\n",
    "* [Appending](#appending)\n",
    "* [Reshaping](#reshaping)\n",
    "* [Scaling](#scaling)\n",
    "\n",
    "And finally:\n",
    "* [Merging Data Frames](#merging)\n",
    "* [Merging GEO-Data Frames and Data Frames](#geomerging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='appending'></a>\n",
    "\n",
    "## Appending\n",
    "\n",
    "As the name implies, this process binds DFs into one, that is, one or more DFs will be put below or on top of another DF. Appending can be done when you fulfill these requisites:\n",
    "1. All the DFs  share the same column names.\n",
    "2. All the DFs  columns are in the same location.\n",
    "\n",
    "Note that it is better if the columns share the same data types. But if they don't, you can solve that during the formatting process.\n",
    "\n",
    "Let's visit this website: https://fundforpeace.org/what-we-do/country-risk-and-fragility-data/\n",
    "\n",
    "There, you will find several excel files with the _Fragile States Index_ per year. Please, create folder **worldData**, and inside of it a folder **fragility**, where you will download the excel files from 2013 to 2021. My folder looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imageForNotebooks/fragilityFiles.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we see all the files in a folder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "path = os.path.join('worldData','fragility','*.xlsx') # xlsx files in the folder\n",
    "excel_files_names = glob.glob(path) #file names using pyhton's glob\n",
    "\n",
    "# see the file names\n",
    "excel_files_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save each data frame in a list: **allDFs**. We will use pandas, but we need **openpyxl** and **xlrd** (for Excel) before doing this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFs=[] # all XLSX will be here!\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "for fileName in excel_files_names:\n",
    "    currentFile=pd.read_excel(fileName)\n",
    "    allDFs.append(currentFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does every data frame in **allDFs** have the same columns? You can not append if they don't. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of rows and columns:\n",
    "for df in allDFs:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspect this would get harder, let's make a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allColumnNames=[] # I will write every column \n",
    "for df in allDFs:\n",
    "    allColumnNames.append(set(df.columns))# list of sets!\n",
    "\n",
    "# this is what we have\n",
    "allColumnNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have an extra column in a couple of years, let's find first the common columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common columns\n",
    "commonColumns=set.intersection(*allColumnNames) # expanding list of sets (*)\n",
    "commonColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the columns not in the common names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonColumns.symmetric_difference(set.union(*allColumnNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide here to leave that one out, so we could make a list of data frames with only the common columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFs_sameNames=[] # final DataFrame (with all DFs from 2013-2021\n",
    "colnamesSorted=sorted(list(commonColumns)) # columns names sorted - must turn 'set' into 'list'\n",
    "\n",
    "# making list of DFs\n",
    "for df in allDFs:\n",
    "    allDFs_sameNames.append(df.loc[:,colnamesSorted]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending in pandas requires a list of data frames, in these case that is **allDFs_sameNames**. Then we proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending\n",
    "allDFsConcat=pd.concat(allDFs_sameNames,ignore_index=True) # appending DFs using 'concat()'\n",
    "\n",
    "#done!... see it:\n",
    "allDFsConcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could pay attention to the current data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns **Year** and **Rank** were expected to be of numeric type, but we got the _object_ type instead. Let's explore *Year*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_counts can be used in object type\n",
    "allDFsConcat.Year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for the year 2021, the other values are in date-time format. We just need an integer number, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping just the year value\n",
    "yearAsNumber=[]\n",
    "for y in allDFsConcat.Year:\n",
    "    try:\n",
    "        yearAsNumber.append(y.year)# the value from a date-time format\n",
    "    except:\n",
    "        yearAsNumber.append(y) # if not a datetime\n",
    "\n",
    "#verifying\n",
    "pd.Series(yearAsNumber).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwriting the year column\n",
    "allDFsConcat['Year']=yearAsNumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed the column ordering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's move 'Country','Year','Total' to the left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a trick: setting columns as index\n",
    "allDFsConcat.set_index(['Country','Year','Total'],inplace=True)\n",
    "allDFsConcat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I will not use _Rank_, I will get rid of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unneeded column\n",
    "allDFsConcat.drop(columns='Rank',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put the row indexes back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes will be columns\n",
    "allDFsConcat.reset_index(drop=False,inplace=True)\n",
    "\n",
    "# see\n",
    "allDFsConcat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some cleaning on the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see column names\n",
    "allDFsConcat.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean column names\n",
    "allDFsConcat.columns=allDFsConcat.columns.str.replace(':\\s',\"_\",regex=True)\n",
    "allDFsConcat.columns=allDFsConcat.columns.str.replace('\\s',\"\",regex=True)\n",
    "#see\n",
    "allDFsConcat.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the country names into upper case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwriting country\n",
    "allDFsConcat['Country']=allDFsConcat.Country.str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again the format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **allDFsConcat** looks ok. Let's go into reshaping, where we will find more things to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "\n",
    "______\n",
    "\n",
    "<a id='reshaping'></a>\n",
    "\n",
    "## Reshaping\n",
    "\n",
    "Data frames have have different shapes. Let me keep some columns from the last DF so you can notice something:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing long shape\n",
    "fragileLong=allDFsConcat.iloc[:,:3]\n",
    "fragileLong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice a DF is in long shape when you see the unit of analysis repeated in more than one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country names repeated...\n",
    "fragileLong.sort_values(['Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me turn our **long** into **wide** shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to wide\n",
    "fragileWide=pd.pivot_table(fragileLong,\n",
    "               values='Total', # values to use\n",
    "               index=['Country'], # unit of analysis\n",
    "               columns=['Year']) # the values for NEW column\n",
    "# see wide\n",
    "fragileWide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **wide** shape from a **pivot_table** function looks great, but pay attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragileWide.index.names,fragileWide.columns.names,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, the indexes and columns have names. If you prefer not to keep those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting index, keeping last index as a column\n",
    "fragileWide= fragileWide.reset_index(drop=False).\\\n",
    "             rename_axis(index=None, columns=None) # no name for indexes\n",
    "\n",
    "# result:\n",
    "fragileWide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notoce that *Long shape* is efficient in the presence of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values in long format\n",
    "fragileLong[fragileLong.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, *Wide shape* may create missing values after reshaping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what cells have missing values?\n",
    "fragileWide[fragileWide.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last request shows us problems.\n",
    "As you can see, even though the DFs were prepared by the same organization, the DFs have country names that differ among different years. Here we need some **manual** changes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare changes as dict:\n",
    "changes={\"CABO VERDE\": \"CAPE VERDE\",\n",
    "\"CZECHIA\":\"CZECH REPUBLIC\",\n",
    "\"SWAZILAND\":\"ESWATINI\",\n",
    "\"ISRAEL AND WEST BANK\":\"ISRAEL\",\n",
    "\"KYRGYZSTAN\":\"KYRGYZ REPUBLIC\",\n",
    "\"NORTH MACEDONIA\":\"MACEDONIA\",\n",
    "\"SLOVAKIA\": \"SLOVAK REPUBLIC\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice I am making the changes in **allDFsConcat** before re creating the long shape DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make changes using 'replace':\n",
    "allDFsConcat.Country.replace(to_replace=changes,inplace=True)\n",
    "# re create:\n",
    "fragileLong=allDFsConcat.iloc[:,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redo the wide reshape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to wide shape again\n",
    "fragileWide=pd.pivot_table(fragileLong,\n",
    "               values='Total',\n",
    "               index=['Country'],\n",
    "               columns=['Year']).\\\n",
    "            reset_index(drop=False).\\\n",
    "            rename_axis(index=None, columns=None)\n",
    "\n",
    "# verify missing\n",
    "fragileWide[fragileWide.isna().any(axis=1)] # remember you had an extra country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to be very careful when working with countries, specially when you are including or excluding countries; which may cause you hurting someone else's feelings. \n",
    "\n",
    "However, should we drop rows, we should reset the row indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragileWide.reset_index(drop=True,inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sure, we can turn this wide shape into a long one, using the function **melt**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(fragileWide, id_vars=['Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragileLong=pd.melt(fragileWide, #DF\n",
    "        id_vars=['Country'], #key\n",
    "        value_vars=list(range(2013,2022)), # columns in wide\n",
    "        var_name='Year', # new name for long column\n",
    "        value_name='Total')# new name for values\n",
    "\n",
    "# verifying\n",
    "fragileLong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you just saw, while doing the reshaping we solved more problems in the **allDFsConcat**. Let's move into scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "\n",
    "\n",
    "______\n",
    "\n",
    "<a id='scaling'></a>\n",
    "\n",
    "\n",
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It all look great so far. However, once you think you have the data ready, you should see the data ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **describe** will only show numerical stats by default, so you need the parameter _include_ set to *all*. However, for our case, we should just request the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.iloc[:,2:].describe().loc[['min','max']].T # notice the transposing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A boxplot may also be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "allDFsConcat.iloc[:,2:].plot(kind='box', rot=90)\n",
    "#plt.semilogy();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, the range of Total is very different from the rest. Let's make sure that this column shares the same range as the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToScale=['Total'] # you can add more columns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# prepare the process\n",
    "minmaxSc = preprocessing.MinMaxScaler(feature_range=(0, 10))# default is 0,1\n",
    "\n",
    "# apply process\n",
    "minmaxResult = minmaxSc.fit_transform(allDFsConcat[columnsToScale])\n",
    "\n",
    "# result\n",
    "minmaxResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new values to new column\n",
    "allDFsConcat['Total_minMax']=minmaxResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.iloc[:,3:].plot(kind='box', rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way would be to standardize all these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.iloc[:,3:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the process\n",
    "stdSc = preprocessing.StandardScaler()\n",
    "\n",
    "# apply process\n",
    "stdScResult = stdSc.fit_transform(allDFsConcat.iloc[:,2:-1])\n",
    "\n",
    "# result\n",
    "stdScResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy as pandas\n",
    "pd.DataFrame(stdScResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stdScResult).plot(kind='box', rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need names for those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newNames_sd=[name+'_sd' for name in allDFsConcat.iloc[:,2:-1].columns]\n",
    "newNames_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me use that array to replace my values in the pandas _Series_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stdScResult,columns=newNames_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame\n",
    "stDF=pd.DataFrame(stdScResult,columns=newNames_sd)\n",
    "\n",
    "# append to the end (right)\n",
    "allDFsConcat=pd.concat([allDFsConcat,stDF],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these are my new data values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.iloc[:,3:].plot(kind='box', rot=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new columns have different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.loc[:,['Total','Total_minMax','Total_sd']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But are prefectly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table\n",
    "allDFsConcat.loc[:,['Total','Total_minMax','Total_sd']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "pd.plotting.scatter_matrix(allDFsConcat.loc[:,['Total','Total_minMax','Total_sd']], alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me save **allDFsConcat**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFsConcat.to_csv(os.path.join(\"data\",\"allDFsConcat.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "\n",
    "______\n",
    "\n",
    "<a id='merging'></a>\n",
    "\n",
    "## Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging data sets need the following considerations:\n",
    "\n",
    "* Merging is done on two data frames.\n",
    "* You need columns in each data frame that share the same exact and unique values. The column names or titles need not be the same. In general, it is only one, but a combination of columns is possible.\n",
    "* The merged table shows by default the mutual coincidences; but you can also request the values not matched, which will help you detect possible extra cleaning.\n",
    "* Pandas jargon uses a **left** and a **right** data frame: **left**.merge(**right**).\n",
    "\n",
    "Let me keep the data for 2021 in our fragility data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragility2021=allDFsConcat[allDFsConcat.Year==2021]\n",
    "fragility2021.drop(columns='Year',inplace=True)\n",
    "fragility2021.reset_index(drop=True,inplace=True)\n",
    "fragility2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let me bring another data from the web with [country codes](https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to install 'html5lib', 'beautifulSoup4' and 'lxml'\n",
    "\n",
    "codesLink='https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes'\n",
    "\n",
    "allTablesWiki=pd.read_html(codesLink, flavor='bs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object **allTablesWiki** is a list of data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what and how many\n",
    "type(allTablesWiki), len(allTablesWiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have five DFs. Which is the one we need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just guessing\n",
    "allTablesWiki[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was the right guess!\n",
    "\n",
    "Now, let's keep that one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes=allTablesWiki[0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice they are **MultiIndex**. Let's flatten the columns names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "['_'.join(col) for col in countryCodes.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2\n",
    "[col[1] for col in countryCodes.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping option 2\n",
    "countryCodes.columns=[col[1] for col in countryCodes.columns.values]\n",
    "countryCodes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current names are not nice yet. Let's use some functions to make them look nicer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide\n",
    "countryCodes.columns.str.split(\"[\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide and keep\n",
    "[element[0] for element in countryCodes.columns.str.split(\"[\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide,  keep and titling\n",
    "[element[0].title() for element in countryCodes.columns.str.split(\"[\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide,  keep , titling and replace\n",
    "[element[0].title().replace(\" \",\"\") for element in countryCodes.columns.str.split(\"[\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide,  keep , titling and replace (and more replace)\n",
    "[element[0].title().replace(\" \",\"\").replace(\"-\",\"\") for element in countryCodes.columns.str.split(\"[\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last version looks good enough. However, this might have been easier using **REGEX**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes.columns.str.title().str.replace('\\[\\w+\\]|\\s|\\-',\"\",regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes.columns=countryCodes.columns.str.title().str.replace('\\[\\w+\\]|\\s|\\-',\"\",regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop some columns and rename the code columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes.drop(columns=[\"SubdivisionCodeLinks\"],inplace=True)\n",
    "countryCodes.rename(columns={'Alpha2Code':'iso2','Alpha3Code':'iso3','InternetCctld':'internet'},inplace=True)\n",
    "countryCodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me create a new column, but without accents and with country names in upper case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bye accents\n",
    "countryCodes['Country']=countryCodes['CountryName'].str.normalize('NFKD').\\\n",
    "                        str.encode('ascii', errors='ignore').str.decode('utf-8').str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the current situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a row with missing values in iso2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes[countryCodes.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas interpreted the iso2 of a country as a missing value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes.loc[countryCodes.Country=='NAMIBIA','iso2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy fix\n",
    "countryCodes.loc[countryCodes.Country=='NAMIBIA','iso2']=\"NA\"\n",
    "\n",
    "# something missing?\n",
    "countryCodes[countryCodes.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could verify the amount of characters in iso2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are these iso2 valid values?\n",
    "[x for x in countryCodes.iso2 if len(x)>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should not be possible. Let's check those rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badValues=[x for x in countryCodes.iso2 if len(x)>2]\n",
    "\n",
    "countryCodes[countryCodes.iso2.isin(badValues)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of those rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes=countryCodes[~countryCodes.iso2.isin(badValues)] # filtering\n",
    "\n",
    "countryCodes.reset_index(drop=True,inplace=True) # needed when rows are dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue with merging. But before that, keep these numbers in mind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragility2021.shape,countryCodes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show you some merge approaches, but I will only show the amount of columns produced:\n",
    "\n",
    "1. You keep only what is common in both key columns:\n",
    "\n",
    "This is the default. The final rows will be the ones where the key values in each data frame match exactly. In this case, your count of rows will be at most the amount of rows of the smallest data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many resulting rows after inner merging\n",
    "fragility2021.merge(countryCodes,how='inner',left_on='Country',right_on='Country').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You keep all the keys from one data frame:\n",
    "\n",
    "The final rows will be all the rows from the dataframe (here from the _left_). If a key values does not find a match, the key value is kept, but the columns will have missing values. In this case, your count of rows will be equal to the amount of rows of the data frame to the left. You can also use **right** so the same logic applies to the data frame to the right.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many resulting rows after left merging\n",
    "fragility2021.merge(countryCodes,how='left',left_on='Country',right_on='Country').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You keep all the rows from both data frames:\n",
    "\n",
    "In this case you will obtain all possible rows: the matched values, and the unmatched values from both data frames. You will also generate missing values. In this case, your count of rows will be at least the amount of rows of the data frame with the most rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many resulting rows after outer merging\n",
    "fragility2021.merge(countryCodes,how='outer',left_on='Country',right_on='Country').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the **inner** merge on the CIA data files this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default is inner merge\n",
    "fragility2021_iso=fragility2021.merge(countryCodes) # notice less argument\n",
    "fragility2021_iso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Merge\n",
    "\n",
    "Merging is done. \n",
    "\n",
    "Can we improve it?\n",
    "\n",
    "We use fuzzy merging when we know that we could improve merge by changing the values in the key column so that more rows are matched. However, the algorithm can get confused if we have **noise** in the data. Let's look for noise:\n",
    "\n",
    "Let's pay attention to the **Sovereignty** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe there are repeated countries because some territories are beyond their borders..?\n",
    "countryCodes.Sovereignty.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have duplicates in column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(countryCodes.Country)),len(countryCodes.Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes[countryCodes.Country.str.contains('UNITED|FRANCE|NETHER|AUSTRALIA|CHINA')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we will look for **similarities**, so let's solve the USA case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes.loc[236,'Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragility2021[fragility2021.Country.str.contains('UNITED STATES')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then\n",
    "countryCodes.loc[236,'Country']='UNITED STATES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find what are the country names that are not shared between those DFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries in 'countryCodes' but NOT in 'fragility2021' \n",
    "OnlyCodes=set(countryCodes.Country)-set(fragility2021.Country)\n",
    "OnlyCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries in 'fragility2021'  but NOT in 'countryCodes'\n",
    "OnlyFragility=set(fragility2021.Country)-set(countryCodes.Country)\n",
    "OnlyFragility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we should try to find what countries in _OnlyFragility_ may match the ones in _OnlyCodes_. \n",
    "\n",
    "We need to use the **fuzzy merge** approach (please install **thefuzz** if not previously installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import process as fz\n",
    "\n",
    "# take a country from OnlyFragility\n",
    "# look for a country in OnlyCodes and return the most similar\n",
    "[(f,fz.extractOne(f, OnlyCodes)) for f in sorted(OnlyFragility)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you have found _some_ good matches. Let's keep the best ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(f,fz.extractOne(f, OnlyCodes)) for f in sorted(OnlyFragility)\n",
    " if fz.extractOne(f, OnlyCodes)[1]>=90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have good matches, you have to create a dictionary like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesCodes1={fz.extractOne(f, OnlyCodes)[0]:f \n",
    "                 for f in sorted(OnlyFragility)\n",
    "                 if fz.extractOne(f, OnlyCodes)[1] >=90}\n",
    "#dict of matches\n",
    "changesCodes1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use that dict for the replacements in *countryCodes*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes.Country.replace(to_replace=changesCodes1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This process can be done a few more times, and you can recover more rows for the merging process. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second try\n",
    "OnlyCodes=set(countryCodes.Country)-set(fragility2021.Country)\n",
    "OnlyFragility=set(fragility2021.Country)-set(countryCodes.Country)\n",
    "[(f,fz.extractOne(f, OnlyCodes)) for f in sorted(OnlyFragility)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might need some manual changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCodes[countryCodes.Country.str.contains('LAO|KOREA|CZECH',regex=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of repetitive strings confuses the fuzzy algorithm. Then, we could get rid of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solving manually KOREA:\n",
    "countryCodes.loc[118,'Country']='NORTH KOREA'\n",
    "countryCodes.loc[119,'Country']='SOUTH KOREA'\n",
    "countryCodes.loc[122,'Country']='LAOS'\n",
    "countryCodes.loc[59,'Country']='CZECH REPUBLIC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again\n",
    "OnlyCodes=set(countryCodes.Country)-set(fragility2021.Country)\n",
    "OnlyFragility=set(fragility2021.Country)-set(countryCodes.Country)\n",
    "[(f,fz.extractOne(f, OnlyCodes)) for f in sorted(OnlyFragility)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second dict of changes\n",
    "changesCodes2={fz.extractOne(f, OnlyCodes)[0]:f \n",
    "                 for f in sorted(OnlyFragility)\n",
    "                 if fz.extractOne(f, OnlyCodes)[1] >=52}\n",
    "#dict of matches\n",
    "changesCodes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the changes\n",
    "countryCodes.Country.replace(to_replace=changesCodes2,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last try?\n",
    "OnlyCodes=set(countryCodes.Country)-set(fragility2021.Country)\n",
    "OnlyFragility=set(fragility2021.Country)-set(countryCodes.Country)\n",
    "[(f,fz.extractOne(f, OnlyCodes)) for f in sorted(OnlyFragility)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready for the merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragility2021_iso=fragility2021.merge(countryCodes) #merge on Country\n",
    "fragility2021_iso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have recovered all the rows to match the amount of rows in *fragility2021*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking:\n",
    "fragility2021_iso.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo Merging\n",
    "\n",
    "Let me fetch a world map from this \n",
    "[website](https://public.opendatasoft.com/explore/dataset/world-administrative-boundaries/export/). Download the **geojson** format and save it your *maps* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "world=gpd.read_file(os.path.join(\"maps\",\"world-administrative-boundaries.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our file\n",
    "world.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the crs info\n",
    "world.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any missing?\n",
    "world[world.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map with no missing:\n",
    "\n",
    "worldFull=world[~world.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you merge a GDF with a DF, **the GDF has to be on the left**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMap=worldFull.merge(fragility2021_iso,on='iso3')\n",
    "# here it is:\n",
    "theMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice your merge brought two more rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates?\n",
    "theMap[pd.Series(theMap.iso3).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more detail\n",
    "theMap[theMap.iso3.str.contains('PSE|PRT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a look:\n",
    "theMap[theMap.iso3.str.contains('PSE|PRT')].explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMap.to_file(os.path.join(\"maps\",\"worlMapData.gpkg\"), layer='countries', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to color our maps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choropleths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me take a look at the Total variable (_Total_minMax_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMap['Total_minMax'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMap.Total_minMax.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot tells you the distribution of the values, but not the presence of outliers, which you are revealed in a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMap.boxplot(column=['Total_minMax'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the histogram divides the data in intervals which are the base of the bars. Seaborn uses the [Freedman-Diaconis](https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule) formula to compute the bins.\n",
    "\n",
    "Let's see other possibilities, but please install [**numba**](https://numba.readthedocs.io/en/stable/user/installing.html) before runing the next code; also make sure you have **pysal**, **mapclassify** and **numpy** installed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mapclassify \n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12345) # so we all get the same results!\n",
    "\n",
    "# let's try 5 intervals\n",
    "K=5\n",
    "theVar=theMap.Total_minMax\n",
    "# same interval width, easy interpretation\n",
    "ei5 = mapclassify.EqualInterval(theVar, k=K)\n",
    "# same interval width based on standard deviation, easy - but not as the previous one, poor when high skewness\n",
    "msd = mapclassify.StdMean(theVar)\n",
    "# interval width varies, counts per interval are close, not easy to grasp, repeated values complicate cuts                                \n",
    "q5=mapclassify.Quantiles(theVar,k=K)\n",
    "\n",
    "# based on similarity, good for multimodal data \n",
    "mb5 = mapclassify.MaximumBreaks(theVar, k=K)\n",
    "# based on similarity, good for skewed data\n",
    "ht = mapclassify.HeadTailBreaks(theVar) # no K needed\n",
    "# based on similarity, optimizer\n",
    "fj5 = mapclassify.FisherJenks(theVar, k=K)\n",
    "# based on similarity, optimizer\n",
    "jc5 = mapclassify.JenksCaspall(theVar, k=K)\n",
    "# based on similarity, optimizer\n",
    "mp5 = mapclassify.MaxP(theVar, k=K) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the **HeadTailBreaks** results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a frequency table by default\n",
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group label\n",
    "ht.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels and counts\n",
    "np.unique(ht.yb,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ht.yb into a pandas Series\n",
    "\n",
    "pd.Series(ht.yb).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the cuts, but the min value is not including \n",
    "ht.bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completing the bins\n",
    "HT_bins=list(ht.bins)\n",
    "HT_bins.insert(0,theVar.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMap.Total_minMax.hist(bins=HT_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we select the right classification?\n",
    "Let me use the the Absolute deviation around class median (ADCM) to make the comparisson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class5 = q5, ei5,msd, ht, mb5, fj5, jc5, mp5\n",
    "# Collect ADCM for each classifier\n",
    "fits = np.array([ c.adcm for c in class5])\n",
    "# Convert ADCM scores to a DataFrame\n",
    "adcms = pd.DataFrame(fits)\n",
    "# Add classifier names\n",
    "adcms['classifier'] = [c.name for c in class5]\n",
    "# Add column names to the ADCM\n",
    "adcms.columns = ['ADCM', 'Classifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the **adcms**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adcms.sort_values('ADCM').plot.barh(x='Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me keep the three best options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMap['Total_ei5'] = ei5.yb\n",
    "theMap['Total_fj5'] = fj5.yb\n",
    "theMap['Total_jc5'] = jc5.yb\n",
    "\n",
    "# we have:\n",
    "theMap[['Total_ei5','Total_fj5','Total_jc5']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many countries we have per class, in each scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class5new = ei5,fj5, jc5\n",
    "pd.DataFrame(\n",
    "    {c.name: c.counts for c in class5new},\n",
    "    index=['Class-{}'.format(i) for i in range(5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot _Total_ei5_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "theMap.plot(column='Total_ei5', \n",
    "        cmap='viridis', \n",
    "        categorical=True,\n",
    "        edgecolor='white', \n",
    "        linewidth=0., \n",
    "        alpha=0.75, \n",
    "        legend=True,\n",
    "        legend_kwds={'loc':3},\n",
    "        ax=ax\n",
    "       )\n",
    "\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all the schemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn \"enumerate()\":\n",
    "varsToPlot = ['Total_ei5','Total_fj5','Total_jc5']\n",
    "[x for x in enumerate(varsToPlot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3 figures\n",
    "f, axs = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\n",
    "\n",
    "axs = axs.flatten() # just a one dimensional index\n",
    "\n",
    "for i, col in enumerate(varsToPlot):    \n",
    "    HERE = axs[i]# select the axis where the map will go\n",
    "    theMap.plot(column=col, categorical=True, linewidth=0.1,\n",
    "                legend=True,cmap='hot_r',edgecolor='grey',\n",
    "                legend_kwds={'loc':3},\n",
    "                ax=HERE)\n",
    "    # Remove axis clutter\n",
    "    ax.set_axis_off()\n",
    "    # Set the axis title to the name of variable being plotted\n",
    "    ax.set_title(col)\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "theMap.to_file(os.path.join(\"maps\",\"worlMapData.gpkg\"), layer='countries', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep one continent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMap.continent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "americaMap=theMap[theMap.continent=='Americas']\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(12, 8))\n",
    "americaMap.plot(ax=ax)\n",
    "\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is projected\n",
    "\n",
    "americaMap.crs.axis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reprojecting\n",
    "americaMap=americaMap.to_crs(8858)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's request the centroid of every country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "americaMap['centroid']=americaMap.geometry.centroid\n",
    "americaMap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "americaMap.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just the plot\n",
    "base=americaMap.plot(facecolor='white',\n",
    "                     edgecolor='lightgrey',\n",
    "                     linewidth=0.3,\n",
    "                     figsize=(8,8))\n",
    "\n",
    "# another column, with the size of the POINTS (based on \"Total_ei5\")\n",
    "americaMap[\"sizeOfMarker_ei5\"]=[4**exp for exp in americaMap[\"Total_ei5\"]]\n",
    "\n",
    "# now, plotting the centroids (declared as the geometry)!!!\n",
    "americaMap.set_geometry(\"centroid\").plot(column=\"Total_ei5\",\n",
    "                                         markersize=americaMap[\"sizeOfMarker_ei5\"],\n",
    "                                         cmap='Paired_r',\n",
    "                                         legend=True,ax=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facetting\n",
    "f, axs = plt.subplots(nrows=2, ncols=3, figsize=(8, 8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i in range(5):  \n",
    "    ax = axs[i]# select the axis where the map will go\n",
    "    americaMap.plot(ax=ax,color='gainsboro')\n",
    "    americaMap[americaMap.Total_ei5==i].plot(ax=ax,color='red')\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"level_\"+str(i))\n",
    "f.delaxes(axs[5]) # bye empty subplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
